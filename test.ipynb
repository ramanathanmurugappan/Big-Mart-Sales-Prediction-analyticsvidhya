{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-23 17:18:19,486] A new study created in memory with name: no-name-466a23e1-ba77-4fdb-b6d6-96e6a0f9bae7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features selected: 7\n",
      "Selected features: ['Item_MRP', 'Item_Fat_Content_Low Fat', 'Item_Type_Fruits and Vegetables', 'Item_Type_Seafood', 'Outlet_Type_Grocery Store', 'Outlet_Type_Supermarket Type2', 'Outlet_Type_Supermarket Type3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-23 17:18:24,118] Trial 0 finished with value: 1263.7668630241294 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 152, 'xgb_max_depth': 14, 'xgb_learning_rate': 0.18432335340553055, 'xgb_reg_alpha': 0.7080725777960455, 'xgb_reg_lambda': 0.020584494295802447}. Best is trial 0 with value: 1263.7668630241294.\n",
      "[I 2025-02-23 17:18:28,876] Trial 1 finished with value: 1079.0800961535354 and parameters: {'model': 'RandomForest', 'rf_n_estimators': 572, 'rf_max_depth': 8, 'rf_min_samples_leaf': 36}. Best is trial 1 with value: 1079.0800961535354.\n",
      "[I 2025-02-23 17:18:56,090] Trial 2 finished with value: 1204.0546481710796 and parameters: {'model': 'GradientBoosting', 'gb_n_estimators': 279, 'gb_max_depth': 9, 'gb_learning_rate': 0.18180022496999232, 'gb_min_samples_leaf': 14}. Best is trial 1 with value: 1079.0800961535354.\n",
      "[I 2025-02-23 17:18:57,330] Trial 3 finished with value: 1092.673847557283 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 374, 'et_max_depth': 4, 'et_min_samples_leaf': 72}. Best is trial 1 with value: 1079.0800961535354.\n",
      "[I 2025-02-23 17:19:00,202] Trial 4 finished with value: 1074.9391969762387 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 696, 'et_max_depth': 7, 'et_min_samples_leaf': 57}. Best is trial 4 with value: 1074.9391969762387.\n",
      "[I 2025-02-23 17:19:04,677] Trial 5 finished with value: 1142.303189602529 and parameters: {'model': 'LightGBM', 'lgb_n_estimators': 638, 'lgb_max_depth': 14, 'lgb_learning_rate': 0.03566282559505665, 'lgb_reg_alpha': 0.1959828624191452, 'lgb_reg_lambda': 0.045227288910538066}. Best is trial 4 with value: 1074.9391969762387.\n",
      "[I 2025-02-23 17:19:07,193] Trial 6 finished with value: 1146.7169643864675 and parameters: {'model': 'CatBoost', 'cat_iterations': 588, 'cat_depth': 4, 'cat_learning_rate': 0.2426371244186715, 'cat_l2_leaf_reg': 1.6709557931179373}. Best is trial 4 with value: 1074.9391969762387.\n",
      "[I 2025-02-23 17:19:14,129] Trial 7 finished with value: 1088.6982170201377 and parameters: {'model': 'RandomForest', 'rf_n_estimators': 756, 'rf_max_depth': 13, 'rf_min_samples_leaf': 16}. Best is trial 4 with value: 1074.9391969762387.\n",
      "[I 2025-02-23 17:19:16,198] Trial 8 finished with value: 1222.3909163621495 and parameters: {'model': 'LightGBM', 'lgb_n_estimators': 380, 'lgb_max_depth': 7, 'lgb_learning_rate': 0.22158579171803858, 'lgb_reg_alpha': 0.6375574713552131, 'lgb_reg_lambda': 0.8872127425763265}. Best is trial 4 with value: 1074.9391969762387.\n",
      "[I 2025-02-23 17:20:09,558] Trial 9 finished with value: 1222.6564495060934 and parameters: {'model': 'GradientBoosting', 'gb_n_estimators': 544, 'gb_max_depth': 9, 'gb_learning_rate': 0.1339868953239794, 'gb_min_samples_leaf': 12}. Best is trial 4 with value: 1074.9391969762387.\n",
      "[I 2025-02-23 17:20:14,864] Trial 10 finished with value: 1077.8709643566094 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 901, 'et_max_depth': 11, 'et_min_samples_leaf': 24}. Best is trial 4 with value: 1074.9391969762387.\n",
      "[I 2025-02-23 17:20:20,952] Trial 11 finished with value: 1082.7896814460867 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 925, 'et_max_depth': 12, 'et_min_samples_leaf': 13}. Best is trial 4 with value: 1074.9391969762387.\n",
      "[I 2025-02-23 17:20:24,938] Trial 12 finished with value: 1074.6732943203172 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 815, 'et_max_depth': 8, 'et_min_samples_leaf': 40}. Best is trial 12 with value: 1074.6732943203172.\n",
      "[I 2025-02-23 17:20:27,928] Trial 13 finished with value: 1075.9059522019602 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 646, 'et_max_depth': 6, 'et_min_samples_leaf': 50}. Best is trial 12 with value: 1074.6732943203172.\n",
      "[I 2025-02-23 17:20:31,448] Trial 14 finished with value: 1074.8162092651778 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 658, 'et_max_depth': 8, 'et_min_samples_leaf': 51}. Best is trial 12 with value: 1074.6732943203172.\n",
      "[I 2025-02-23 17:20:36,480] Trial 15 finished with value: 1078.8584342536942 and parameters: {'model': 'CatBoost', 'cat_iterations': 120, 'cat_depth': 10, 'cat_learning_rate': 0.03355497508874883, 'cat_l2_leaf_reg': 9.742304653893186}. Best is trial 12 with value: 1074.6732943203172.\n",
      "[I 2025-02-23 17:20:39,739] Trial 16 finished with value: 1097.2423270547238 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 987, 'xgb_max_depth': 3, 'xgb_learning_rate': 0.03275409164036641, 'xgb_reg_alpha': 0.0053658066640442925, 'xgb_reg_lambda': 0.9993624386415516}. Best is trial 12 with value: 1074.6732943203172.\n",
      "[I 2025-02-23 17:20:40,437] Trial 17 finished with value: 1075.1760827355606 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 123, 'et_max_depth': 9, 'et_min_samples_leaf': 40}. Best is trial 12 with value: 1074.6732943203172.\n",
      "[I 2025-02-23 17:20:43,844] Trial 18 finished with value: 1078.1469714043274 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 738, 'et_max_depth': 15, 'et_min_samples_leaf': 94}. Best is trial 12 with value: 1074.6732943203172.\n",
      "[I 2025-02-23 17:20:46,036] Trial 19 finished with value: 1074.6657087616481 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 472, 'et_max_depth': 8, 'et_min_samples_leaf': 40}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:20:47,764] Trial 20 finished with value: 1177.72147860225 and parameters: {'model': 'LightGBM', 'lgb_n_estimators': 866, 'lgb_max_depth': 3, 'lgb_learning_rate': 0.2985626806934205, 'lgb_reg_alpha': 0.9692698520487181, 'lgb_reg_lambda': 0.36014760693883635}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:20:49,909] Trial 21 finished with value: 1074.7966856516973 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 440, 'et_max_depth': 8, 'et_min_samples_leaf': 36}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:20:51,994] Trial 22 finished with value: 1076.092127968067 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 397, 'et_max_depth': 10, 'et_min_samples_leaf': 33}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:20:53,856] Trial 23 finished with value: 1075.8295187059423 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 446, 'et_max_depth': 6, 'et_min_samples_leaf': 32}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:20:56,262] Trial 24 finished with value: 1074.7931623375398 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 508, 'et_max_depth': 8, 'et_min_samples_leaf': 43}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:21:23,293] Trial 25 finished with value: 1232.4282297004556 and parameters: {'model': 'CatBoost', 'cat_iterations': 955, 'cat_depth': 9, 'cat_learning_rate': 0.2991119386422094, 'cat_l2_leaf_reg': 6.646778439534709}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:21:24,056] Trial 26 finished with value: 1157.9883700174341 and parameters: {'model': 'RandomForest', 'rf_n_estimators': 151, 'rf_max_depth': 3, 'rf_min_samples_leaf': 97}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:21:29,182] Trial 27 finished with value: 1289.595155502623 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 660, 'xgb_max_depth': 7, 'xgb_learning_rate': 0.2900151157086668, 'xgb_reg_alpha': 0.9789743198977109, 'xgb_reg_lambda': 0.48284864646139214}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:22:03,212] Trial 28 finished with value: 1082.2131801013159 and parameters: {'model': 'GradientBoosting', 'gb_n_estimators': 991, 'gb_max_depth': 3, 'gb_learning_rate': 0.020837122588772405, 'gb_min_samples_leaf': 100}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:22:07,626] Trial 29 finished with value: 1186.7525438256098 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 112, 'xgb_max_depth': 15, 'xgb_learning_rate': 0.029078204244328165, 'xgb_reg_alpha': 0.08591843464581406, 'xgb_reg_lambda': 0.9547595496905663}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:22:08,391] Trial 30 finished with value: 1137.2498244454262 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 243, 'et_max_depth': 3, 'et_min_samples_leaf': 62}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:22:10,817] Trial 31 finished with value: 1074.6850525248644 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 528, 'et_max_depth': 8, 'et_min_samples_leaf': 41}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:22:13,326] Trial 32 finished with value: 1074.8319744777386 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 529, 'et_max_depth': 9, 'et_min_samples_leaf': 44}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:22:15,802] Trial 33 finished with value: 1075.2757555128883 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 553, 'et_max_depth': 7, 'et_min_samples_leaf': 23}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:22:18,020] Trial 34 finished with value: 1075.8000788833008 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 560, 'et_max_depth': 6, 'et_min_samples_leaf': 45}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:24:32,188] Trial 35 finished with value: 1280.9718678040686 and parameters: {'model': 'GradientBoosting', 'gb_n_estimators': 976, 'gb_max_depth': 15, 'gb_learning_rate': 0.2776239993472201, 'gb_min_samples_leaf': 74}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:24:38,687] Trial 36 finished with value: 1082.008038805032 and parameters: {'model': 'RandomForest', 'rf_n_estimators': 964, 'rf_max_depth': 15, 'rf_min_samples_leaf': 81}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:24:42,135] Trial 37 finished with value: 1075.6891952095416 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 801, 'et_max_depth': 8, 'et_min_samples_leaf': 68}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:24:43,842] Trial 38 finished with value: 1077.0273981579228 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 319, 'et_max_depth': 10, 'et_min_samples_leaf': 26}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:24:45,325] Trial 39 finished with value: 1091.2814111315158 and parameters: {'model': 'LightGBM', 'lgb_n_estimators': 201, 'lgb_max_depth': 15, 'lgb_learning_rate': 0.019170280050830685, 'lgb_reg_alpha': 0.030058960125463607, 'lgb_reg_lambda': 0.9873907080700226}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:24:45,788] Trial 40 finished with value: 1095.4876626634366 and parameters: {'model': 'CatBoost', 'cat_iterations': 124, 'cat_depth': 3, 'cat_learning_rate': 0.021353664736052685, 'cat_l2_leaf_reg': 1.8499127995982256}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:24:47,937] Trial 41 finished with value: 1074.9441600758369 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 464, 'et_max_depth': 8, 'et_min_samples_leaf': 35}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:24:50,307] Trial 42 finished with value: 1075.014607257893 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 500, 'et_max_depth': 9, 'et_min_samples_leaf': 40}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:24:52,838] Trial 43 finished with value: 1074.799163969972 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 596, 'et_max_depth': 7, 'et_min_samples_leaf': 46}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:24:54,857] Trial 44 finished with value: 1074.9286692638143 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 427, 'et_max_depth': 8, 'et_min_samples_leaf': 35}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:24:56,077] Trial 45 finished with value: 1079.6404812160877 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 321, 'et_max_depth': 5, 'et_min_samples_leaf': 54}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:24:56,657] Trial 46 finished with value: 1157.9219413415874 and parameters: {'model': 'RandomForest', 'rf_n_estimators': 105, 'rf_max_depth': 3, 'rf_min_samples_leaf': 61}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:24:59,061] Trial 47 finished with value: 1075.3975222982076 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 484, 'et_max_depth': 10, 'et_min_samples_leaf': 39}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:25:13,788] Trial 48 finished with value: 1100.4204485718205 and parameters: {'model': 'GradientBoosting', 'gb_n_estimators': 145, 'gb_max_depth': 15, 'gb_learning_rate': 0.02818761045101975, 'gb_min_samples_leaf': 47}. Best is trial 19 with value: 1074.6657087616481.\n",
      "[I 2025-02-23 17:25:19,506] Trial 49 finished with value: 1252.0861781160474 and parameters: {'model': 'LightGBM', 'lgb_n_estimators': 969, 'lgb_max_depth': 10, 'lgb_learning_rate': 0.13165326064830243, 'lgb_reg_alpha': 0.4699057180996212, 'lgb_reg_lambda': 0.5547845428183139}. Best is trial 19 with value: 1074.6657087616481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 models and their RMSE scores:\n",
      "Model: ExtraTrees, RMSE: 1074.6657087616481, Params: {'et_n_estimators': 472, 'et_max_depth': 8, 'et_min_samples_leaf': 40}\n",
      "Model: ExtraTrees, RMSE: 1074.6732943203172, Params: {'et_n_estimators': 815, 'et_max_depth': 8, 'et_min_samples_leaf': 40}\n",
      "Model: ExtraTrees, RMSE: 1074.6850525248644, Params: {'et_n_estimators': 528, 'et_max_depth': 8, 'et_min_samples_leaf': 41}\n",
      "Validation RMSE with stacking of top 3 models: 1073.4615374250563\n",
      "Submission file created: submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('train_v9rqX0R.csv')  # Replace with actual train file path\n",
    "train_df = train_df[~train_df['Item_Identifier'].isin([\n",
    "    'FDX20', 'FDG33', 'FDW13', 'FDG24', 'DRE49', 'NCY18',\n",
    "    'FDO19', 'FDL34', 'FDO52', 'NCL31', 'FDA04', 'NCQ06',\n",
    "    'FDT07', 'FDL10', 'FDX04', 'FDU19'])]\n",
    "test_df = pd.read_csv('test_AbJTz2l.csv')    # Replace with actual test file path\n",
    "\n",
    "# Combine train and test for preprocessing\n",
    "train_df['source'] = 'train'\n",
    "test_df['source'] = 'test'\n",
    "combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Data Preprocessing\n",
    "combined_df['Item_Weight'] = combined_df.groupby('Item_Type')['Item_Weight'].transform(lambda x: x.fillna(x.mean()))\n",
    "combined_df['Outlet_Size'] = combined_df.groupby('Outlet_Type')['Outlet_Size'].transform(\n",
    "    lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'Medium'))\n",
    "item_visibility_mean = combined_df[combined_df['Item_Visibility'] > 0].groupby('Item_Identifier')['Item_Visibility'].mean()\n",
    "combined_df['Item_Visibility'] = combined_df.apply(\n",
    "    lambda row: item_visibility_mean[row['Item_Identifier']] if row['Item_Visibility'] == 0 else row['Item_Visibility'],\n",
    "    axis=1\n",
    ")\n",
    "combined_df['Item_Visibility'].fillna(combined_df['Item_Visibility'].mean(), inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "combined_df['Outlet_Age'] = 2013 - combined_df['Outlet_Establishment_Year']\n",
    "combined_df['Item_Fat_Content'] = combined_df['Item_Fat_Content'].replace({'LF': 'Low Fat', 'reg': 'Regular', 'low fat': 'Low Fat'})\n",
    "\n",
    "# Encoding categorical variables\n",
    "cat_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n",
    "for col in cat_cols:\n",
    "    combined_df = pd.concat([combined_df, pd.get_dummies(combined_df[col], prefix=col)], axis=1)\n",
    "    combined_df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "combined_df.drop(['Item_Identifier', 'Outlet_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "\n",
    "# Split back into train and test\n",
    "train = combined_df[combined_df['source'] == 'train'].drop('source', axis=1)\n",
    "test = combined_df[combined_df['source'] == 'test'].drop(['source', 'Item_Outlet_Sales'], axis=1)\n",
    "\n",
    "# Prepare features and target\n",
    "X = train.drop('Item_Outlet_Sales', axis=1)\n",
    "y = train['Item_Outlet_Sales']\n",
    "X_test = test\n",
    "\n",
    "# Scale numerical columns with RobustScaler\n",
    "num_cols = ['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Age']  # Only scale numerical columns\n",
    "scaler = RobustScaler()\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "# Define the base model for RFE\n",
    "base_model = ExtraTreesRegressor(n_estimators=472, max_depth=8, min_samples_leaf=40, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Set number of features to keep (half of total features)\n",
    "n_features = X.shape[1] // 2\n",
    "\n",
    "# Set up RFE\n",
    "selector = RFE(estimator=base_model, n_features_to_select=n_features, step=1)\n",
    "\n",
    "# Fit and transform your data\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X.columns[selector.get_support()].tolist()\n",
    "print(f\"Number of features selected: {len(selected_features)}\")\n",
    "print(f\"Selected features: {selected_features}\")\n",
    "\n",
    "# Split train data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Define objective function for Optuna\n",
    "def objective(trial):\n",
    "    model_name = trial.suggest_categorical(\"model\", [\n",
    "        \"RandomForest\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \"ExtraTrees\", \"GradientBoosting\"\n",
    "    ])\n",
    "    \n",
    "    if model_name == \"RandomForest\":\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('rf_n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('rf_max_depth', 3, 15),\n",
    "            'min_samples_leaf': trial.suggest_int('rf_min_samples_leaf', 10, 100),\n",
    "            'n_jobs': -1,\n",
    "            'random_state': 42\n",
    "        }\n",
    "        model = RandomForestRegressor(**params)\n",
    "    \n",
    "    elif model_name == \"XGBoost\":\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('xgb_n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('xgb_max_depth', 3, 15),\n",
    "            'learning_rate': trial.suggest_float('xgb_learning_rate', 0.01, 0.3),\n",
    "            'reg_alpha': trial.suggest_float('xgb_reg_alpha', 0, 1),\n",
    "            'reg_lambda': trial.suggest_float('xgb_reg_lambda', 0, 1),\n",
    "            'random_state': 42\n",
    "        }\n",
    "        model = XGBRegressor(**params)\n",
    "    \n",
    "    elif model_name == \"LightGBM\":\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('lgb_n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('lgb_max_depth', 3, 15),\n",
    "            'learning_rate': trial.suggest_float('lgb_learning_rate', 0.01, 0.3),\n",
    "            'reg_alpha': trial.suggest_float('lgb_reg_alpha', 0, 1),\n",
    "            'reg_lambda': trial.suggest_float('lgb_reg_lambda', 0, 1),\n",
    "            'random_state': 42,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        model = LGBMRegressor(**params)\n",
    "    \n",
    "    elif model_name == \"CatBoost\":\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('cat_iterations', 100, 1000),\n",
    "            'depth': trial.suggest_int('cat_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('cat_learning_rate', 0.01, 0.3),\n",
    "            'l2_leaf_reg': trial.suggest_float('cat_l2_leaf_reg', 1, 10),\n",
    "            'random_seed': 42,\n",
    "            'verbose': 0\n",
    "        }\n",
    "        model = CatBoostRegressor(**params)\n",
    "    \n",
    "    elif model_name == \"ExtraTrees\":\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('et_n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('et_max_depth', 3, 15),\n",
    "            'min_samples_leaf': trial.suggest_int('et_min_samples_leaf', 10, 100),\n",
    "            'n_jobs': -1,\n",
    "            'random_state': 42\n",
    "        }\n",
    "        model = ExtraTreesRegressor(**params)\n",
    "    \n",
    "    elif model_name == \"GradientBoosting\":\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('gb_n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('gb_max_depth', 3, 15),\n",
    "            'learning_rate': trial.suggest_float('gb_learning_rate', 0.01, 0.3),\n",
    "            'min_samples_leaf': trial.suggest_int('gb_min_samples_leaf', 10, 100),\n",
    "            'random_state': 42\n",
    "        }\n",
    "        model = GradientBoostingRegressor(**params)\n",
    "\n",
    "    # Train and evaluate model with cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    trial.set_user_attr('params', {k: v for k, v in trial.params.items() if k != 'model'})\n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# Optimize with Optuna using TPESampler\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=50)  # Matches your run\n",
    "\n",
    "# Get top 3 models from trials\n",
    "trials = sorted(study.trials, key=lambda t: t.value if t.value is not None else float('inf'))\n",
    "top_3_trials = trials[:3]\n",
    "print(\"Top 3 models and their RMSE scores:\")\n",
    "for trial in top_3_trials:\n",
    "    print(f\"Model: {trial.params['model']}, RMSE: {trial.value}, Params: {trial.user_attrs['params']}\")\n",
    "\n",
    "# Initialize top 3 models with best parameters\n",
    "models = []\n",
    "for trial in top_3_trials:\n",
    "    model_name = trial.params['model']\n",
    "    # Remove model-specific prefix from parameter names\n",
    "    params = {k.split('_', 1)[1] if '_' in k else k: v for k, v in trial.params.items() if k != 'model'}\n",
    "    \n",
    "    if model_name == \"RandomForest\":\n",
    "        model = RandomForestRegressor(**params, n_jobs=-1, random_state=42)\n",
    "    elif model_name == \"XGBoost\":\n",
    "        model = XGBRegressor(**params, random_state=42)\n",
    "    elif model_name == \"LightGBM\":\n",
    "        model = LGBMRegressor(**params, random_state=42, verbose=-1)\n",
    "    elif model_name == \"CatBoost\":\n",
    "        # Rename 'iterations' to match CatBoost API\n",
    "        if 'iterations' in params:\n",
    "            params['iterations'] = params.pop('iterations')\n",
    "        model = CatBoostRegressor(**params, random_seed=42, verbose=0)\n",
    "    elif model_name == \"ExtraTrees\":\n",
    "        model = ExtraTreesRegressor(**params, n_jobs=-1, random_state=42)\n",
    "    elif model_name == \"GradientBoosting\":\n",
    "        model = GradientBoostingRegressor(**params, random_state=42)\n",
    "    \n",
    "    models.append((model_name, model))\n",
    "\n",
    "# Stacking with top 3 models\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_preds = {f\"{name}_{i}\": np.zeros(len(X_selected)) for i, (name, _) in enumerate(models)}\n",
    "test_preds = {f\"{name}_{i}\": np.zeros(len(X_test_selected)) for i, (name, _) in enumerate(models)}\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_selected):\n",
    "    X_tr, X_val = X_selected[train_idx], X_selected[val_idx]\n",
    "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    for i, (model_name, model) in enumerate(models):\n",
    "        model.fit(X_tr, y_tr)\n",
    "        oof_preds[f\"{model_name}_{i}\"][val_idx] = model.predict(X_val)\n",
    "        test_preds[f\"{model_name}_{i}\"] += model.predict(X_test_selected) / kf.n_splits\n",
    "\n",
    "# Meta-model (Ridge Regression)\n",
    "meta_features = np.column_stack([oof_preds[key] for key in oof_preds])\n",
    "meta_model = Ridge(alpha=1.0)\n",
    "meta_model.fit(meta_features, y)\n",
    "\n",
    "# Evaluate on validation set\n",
    "# Using the last fold's X_val for simplicity; for full OOF evaluation, adjust accordingly\n",
    "meta_val_preds = meta_model.predict(np.column_stack([model.predict(X_val) for _, model in models]))\n",
    "rmse = np.sqrt(mean_squared_error(y_val, meta_val_preds))\n",
    "print(f\"Validation RMSE with stacking of top 3 models: {rmse}\")\n",
    "\n",
    "# Final test predictions\n",
    "meta_test_preds = meta_model.predict(np.column_stack([test_preds[key] for key in test_preds]))\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame({\n",
    "    'Item_Identifier': test_df['Item_Identifier'],\n",
    "    'Outlet_Identifier': test_df['Outlet_Identifier'],\n",
    "    'Item_Outlet_Sales': meta_test_preds\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created: submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-23 16:45:21,149] A new study created in memory with name: no-name-371f0e81-9f87-424a-acd4-cdff3446150c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features selected: 16\n",
      "Selected features: ['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Age', 'Item_Fat_Content_Low Fat', 'Item_Fat_Content_Regular', 'Item_Type_Fruits and Vegetables', 'Item_Type_Household', 'Outlet_Size_Small', 'Outlet_Location_Type_Tier 1', 'Outlet_Location_Type_Tier 2', 'Outlet_Location_Type_Tier 3', 'Outlet_Type_Grocery Store', 'Outlet_Type_Supermarket Type1', 'Outlet_Type_Supermarket Type2', 'Outlet_Type_Supermarket Type3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-23 16:45:25,385] Trial 0 finished with value: 1263.7668630241294 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 152, 'xgb_max_depth': 14, 'xgb_learning_rate': 0.18432335340553055, 'xgb_reg_alpha': 0.7080725777960455, 'xgb_reg_lambda': 0.020584494295802447}. Best is trial 0 with value: 1263.7668630241294.\n",
      "[I 2025-02-23 16:45:30,213] Trial 1 finished with value: 1079.0800961535354 and parameters: {'model': 'RandomForest', 'rf_n_estimators': 572, 'rf_max_depth': 8, 'rf_min_samples_leaf': 36}. Best is trial 1 with value: 1079.0800961535354.\n",
      "[I 2025-02-23 16:45:57,354] Trial 2 finished with value: 1204.0546481710796 and parameters: {'model': 'GradientBoosting', 'gb_n_estimators': 279, 'gb_max_depth': 9, 'gb_learning_rate': 0.18180022496999232, 'gb_min_samples_leaf': 14}. Best is trial 1 with value: 1079.0800961535354.\n",
      "[I 2025-02-23 16:45:58,580] Trial 3 finished with value: 1092.673847557283 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 374, 'et_max_depth': 4, 'et_min_samples_leaf': 72}. Best is trial 1 with value: 1079.0800961535354.\n",
      "[I 2025-02-23 16:46:01,741] Trial 4 finished with value: 1074.9391969762387 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 696, 'et_max_depth': 7, 'et_min_samples_leaf': 57}. Best is trial 4 with value: 1074.9391969762387.\n",
      "[I 2025-02-23 16:46:08,028] Trial 5 finished with value: 1142.303189602529 and parameters: {'model': 'LightGBM', 'lgb_n_estimators': 638, 'lgb_max_depth': 14, 'lgb_learning_rate': 0.03566282559505665, 'lgb_reg_alpha': 0.1959828624191452, 'lgb_reg_lambda': 0.045227288910538066}. Best is trial 4 with value: 1074.9391969762387.\n",
      "[I 2025-02-23 16:46:10,610] Trial 6 finished with value: 1146.7169643864675 and parameters: {'model': 'CatBoost', 'cat_iterations': 588, 'cat_depth': 4, 'cat_learning_rate': 0.2426371244186715, 'cat_l2_leaf_reg': 1.6709557931179373}. Best is trial 4 with value: 1074.9391969762387.\n",
      "[I 2025-02-23 16:46:18,323] Trial 7 finished with value: 1088.6982170201377 and parameters: {'model': 'RandomForest', 'rf_n_estimators': 756, 'rf_max_depth': 13, 'rf_min_samples_leaf': 16}. Best is trial 4 with value: 1074.9391969762387.\n",
      "[I 2025-02-23 16:46:21,208] Trial 8 finished with value: 1222.3909163621495 and parameters: {'model': 'LightGBM', 'lgb_n_estimators': 380, 'lgb_max_depth': 7, 'lgb_learning_rate': 0.22158579171803858, 'lgb_reg_alpha': 0.6375574713552131, 'lgb_reg_lambda': 0.8872127425763265}. Best is trial 4 with value: 1074.9391969762387.\n",
      "[I 2025-02-23 16:47:14,263] Trial 9 finished with value: 1222.6564495060934 and parameters: {'model': 'GradientBoosting', 'gb_n_estimators': 544, 'gb_max_depth': 9, 'gb_learning_rate': 0.1339868953239794, 'gb_min_samples_leaf': 12}. Best is trial 4 with value: 1074.9391969762387.\n",
      "[I 2025-02-23 16:47:19,045] Trial 10 finished with value: 1077.8709643566094 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 901, 'et_max_depth': 11, 'et_min_samples_leaf': 24}. Best is trial 4 with value: 1074.9391969762387.\n",
      "[I 2025-02-23 16:47:24,424] Trial 11 finished with value: 1082.7896814460867 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 925, 'et_max_depth': 12, 'et_min_samples_leaf': 13}. Best is trial 4 with value: 1074.9391969762387.\n",
      "[I 2025-02-23 16:47:28,117] Trial 12 finished with value: 1074.6732943203172 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 815, 'et_max_depth': 8, 'et_min_samples_leaf': 40}. Best is trial 12 with value: 1074.6732943203172.\n",
      "[I 2025-02-23 16:47:30,667] Trial 13 finished with value: 1075.9059522019602 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 646, 'et_max_depth': 6, 'et_min_samples_leaf': 50}. Best is trial 12 with value: 1074.6732943203172.\n",
      "[I 2025-02-23 16:47:33,660] Trial 14 finished with value: 1074.8162092651778 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 658, 'et_max_depth': 8, 'et_min_samples_leaf': 51}. Best is trial 12 with value: 1074.6732943203172.\n",
      "[I 2025-02-23 16:47:38,540] Trial 15 finished with value: 1078.8584342536942 and parameters: {'model': 'CatBoost', 'cat_iterations': 120, 'cat_depth': 10, 'cat_learning_rate': 0.03355497508874883, 'cat_l2_leaf_reg': 9.742304653893186}. Best is trial 12 with value: 1074.6732943203172.\n",
      "[I 2025-02-23 16:47:41,221] Trial 16 finished with value: 1097.2423270547238 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 987, 'xgb_max_depth': 3, 'xgb_learning_rate': 0.03275409164036641, 'xgb_reg_alpha': 0.0053658066640442925, 'xgb_reg_lambda': 0.9993624386415516}. Best is trial 12 with value: 1074.6732943203172.\n",
      "[I 2025-02-23 16:47:41,915] Trial 17 finished with value: 1075.1760827355606 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 123, 'et_max_depth': 9, 'et_min_samples_leaf': 40}. Best is trial 12 with value: 1074.6732943203172.\n",
      "[I 2025-02-23 16:47:45,326] Trial 18 finished with value: 1078.1469714043274 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 738, 'et_max_depth': 15, 'et_min_samples_leaf': 94}. Best is trial 12 with value: 1074.6732943203172.\n",
      "[I 2025-02-23 16:47:47,541] Trial 19 finished with value: 1074.6657087616484 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 472, 'et_max_depth': 8, 'et_min_samples_leaf': 40}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:47:49,330] Trial 20 finished with value: 1177.72147860225 and parameters: {'model': 'LightGBM', 'lgb_n_estimators': 866, 'lgb_max_depth': 3, 'lgb_learning_rate': 0.2985626806934205, 'lgb_reg_alpha': 0.9692698520487181, 'lgb_reg_lambda': 0.36014760693883635}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:47:51,404] Trial 21 finished with value: 1074.7966856516973 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 440, 'et_max_depth': 8, 'et_min_samples_leaf': 36}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:47:53,524] Trial 22 finished with value: 1076.092127968067 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 397, 'et_max_depth': 10, 'et_min_samples_leaf': 33}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:47:55,413] Trial 23 finished with value: 1075.8295187059423 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 446, 'et_max_depth': 6, 'et_min_samples_leaf': 32}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:47:57,790] Trial 24 finished with value: 1074.7931623375398 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 508, 'et_max_depth': 8, 'et_min_samples_leaf': 43}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:48:24,884] Trial 25 finished with value: 1232.4282297004556 and parameters: {'model': 'CatBoost', 'cat_iterations': 955, 'cat_depth': 9, 'cat_learning_rate': 0.2991119386422094, 'cat_l2_leaf_reg': 6.646778439534709}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:48:25,685] Trial 26 finished with value: 1157.9883700174341 and parameters: {'model': 'RandomForest', 'rf_n_estimators': 151, 'rf_max_depth': 3, 'rf_min_samples_leaf': 97}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:48:31,390] Trial 27 finished with value: 1289.595155502623 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 660, 'xgb_max_depth': 7, 'xgb_learning_rate': 0.2900151157086668, 'xgb_reg_alpha': 0.9789743198977109, 'xgb_reg_lambda': 0.48284864646139214}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:49:06,202] Trial 28 finished with value: 1082.2131801013159 and parameters: {'model': 'GradientBoosting', 'gb_n_estimators': 991, 'gb_max_depth': 3, 'gb_learning_rate': 0.020837122588772405, 'gb_min_samples_leaf': 100}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:49:10,851] Trial 29 finished with value: 1186.7525438256098 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 112, 'xgb_max_depth': 15, 'xgb_learning_rate': 0.029078204244328165, 'xgb_reg_alpha': 0.08591843464581406, 'xgb_reg_lambda': 0.9547595496905663}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:49:11,618] Trial 30 finished with value: 1137.2498244454262 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 243, 'et_max_depth': 3, 'et_min_samples_leaf': 62}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:49:14,069] Trial 31 finished with value: 1074.6850525248644 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 528, 'et_max_depth': 8, 'et_min_samples_leaf': 41}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:49:16,609] Trial 32 finished with value: 1074.8319744777386 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 529, 'et_max_depth': 9, 'et_min_samples_leaf': 44}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:49:19,046] Trial 33 finished with value: 1075.2757555128883 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 553, 'et_max_depth': 7, 'et_min_samples_leaf': 23}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:49:21,316] Trial 34 finished with value: 1075.8000788833008 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 560, 'et_max_depth': 6, 'et_min_samples_leaf': 45}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:51:35,729] Trial 35 finished with value: 1280.9718678040686 and parameters: {'model': 'GradientBoosting', 'gb_n_estimators': 976, 'gb_max_depth': 15, 'gb_learning_rate': 0.2776239993472201, 'gb_min_samples_leaf': 74}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:51:42,329] Trial 36 finished with value: 1082.008038805032 and parameters: {'model': 'RandomForest', 'rf_n_estimators': 964, 'rf_max_depth': 15, 'rf_min_samples_leaf': 81}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:51:45,883] Trial 37 finished with value: 1075.6891952095416 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 801, 'et_max_depth': 8, 'et_min_samples_leaf': 68}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:51:47,613] Trial 38 finished with value: 1077.0273981579228 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 319, 'et_max_depth': 10, 'et_min_samples_leaf': 26}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:51:50,420] Trial 39 finished with value: 1091.2814111315158 and parameters: {'model': 'LightGBM', 'lgb_n_estimators': 201, 'lgb_max_depth': 15, 'lgb_learning_rate': 0.019170280050830685, 'lgb_reg_alpha': 0.030058960125463607, 'lgb_reg_lambda': 0.9873907080700226}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:51:50,914] Trial 40 finished with value: 1095.4876626634366 and parameters: {'model': 'CatBoost', 'cat_iterations': 124, 'cat_depth': 3, 'cat_learning_rate': 0.021353664736052685, 'cat_l2_leaf_reg': 1.8499127995982256}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:51:53,306] Trial 41 finished with value: 1074.9441600758369 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 464, 'et_max_depth': 8, 'et_min_samples_leaf': 35}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:51:56,090] Trial 42 finished with value: 1075.014607257893 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 500, 'et_max_depth': 9, 'et_min_samples_leaf': 40}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:51:58,933] Trial 43 finished with value: 1074.799163969972 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 596, 'et_max_depth': 7, 'et_min_samples_leaf': 46}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:52:01,201] Trial 44 finished with value: 1074.9286692638143 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 427, 'et_max_depth': 8, 'et_min_samples_leaf': 35}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:52:02,540] Trial 45 finished with value: 1079.6404812160877 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 321, 'et_max_depth': 5, 'et_min_samples_leaf': 54}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:52:03,160] Trial 46 finished with value: 1157.9219413415874 and parameters: {'model': 'RandomForest', 'rf_n_estimators': 105, 'rf_max_depth': 3, 'rf_min_samples_leaf': 61}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:52:05,872] Trial 47 finished with value: 1075.3975222982076 and parameters: {'model': 'ExtraTrees', 'et_n_estimators': 484, 'et_max_depth': 10, 'et_min_samples_leaf': 39}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:52:20,871] Trial 48 finished with value: 1100.4204485718205 and parameters: {'model': 'GradientBoosting', 'gb_n_estimators': 145, 'gb_max_depth': 15, 'gb_learning_rate': 0.02818761045101975, 'gb_min_samples_leaf': 47}. Best is trial 19 with value: 1074.6657087616484.\n",
      "[I 2025-02-23 16:52:28,379] Trial 49 finished with value: 1252.0861781160474 and parameters: {'model': 'LightGBM', 'lgb_n_estimators': 969, 'lgb_max_depth': 10, 'lgb_learning_rate': 0.13165326064830243, 'lgb_reg_alpha': 0.4699057180996212, 'lgb_reg_lambda': 0.5547845428183139}. Best is trial 19 with value: 1074.6657087616484.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top models from 3 distinct algorithms:\n",
      "Model: ExtraTrees, RMSE: 1074.6657087616484, Params: {'et_n_estimators': 472, 'et_max_depth': 8, 'et_min_samples_leaf': 40}\n",
      "Model: CatBoost, RMSE: 1078.8584342536942, Params: {'cat_iterations': 120, 'cat_depth': 10, 'cat_learning_rate': 0.03355497508874883, 'cat_l2_leaf_reg': 9.742304653893186}\n",
      "Model: RandomForest, RMSE: 1079.0800961535354, Params: {'rf_n_estimators': 572, 'rf_max_depth': 8, 'rf_min_samples_leaf': 36}\n",
      "Validation RMSE with stacking of top models from top 3 algorithms: 1069.1012497284676\n",
      "Submission file created: submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('train_v9rqX0R.csv')  # Replace with actual train file path\n",
    "train_df = train_df[~train_df['Item_Identifier'].isin([\n",
    "    'FDX20', 'FDG33', 'FDW13', 'FDG24', 'DRE49', 'NCY18',\n",
    "    'FDO19', 'FDL34', 'FDO52', 'NCL31', 'FDA04', 'NCQ06',\n",
    "    'FDT07', 'FDL10', 'FDX04', 'FDU19'])]\n",
    "test_df = pd.read_csv('test_AbJTz2l.csv')    # Replace with actual test file path\n",
    "\n",
    "# Combine train and test for preprocessing\n",
    "train_df['source'] = 'train'\n",
    "test_df['source'] = 'test'\n",
    "combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Data Preprocessing\n",
    "combined_df['Item_Weight'] = combined_df.groupby('Item_Type')['Item_Weight'].transform(lambda x: x.fillna(x.mean()))\n",
    "combined_df['Outlet_Size'] = combined_df.groupby('Outlet_Type')['Outlet_Size'].transform(\n",
    "    lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'Medium'))\n",
    "item_visibility_mean = combined_df[combined_df['Item_Visibility'] > 0].groupby('Item_Identifier')['Item_Visibility'].mean()\n",
    "combined_df['Item_Visibility'] = combined_df.apply(\n",
    "    lambda row: item_visibility_mean[row['Item_Identifier']] if row['Item_Visibility'] == 0 else row['Item_Visibility'],\n",
    "    axis=1\n",
    ")\n",
    "combined_df['Item_Visibility'].fillna(combined_df['Item_Visibility'].mean(), inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "combined_df['Outlet_Age'] = 2025 - combined_df['Outlet_Establishment_Year']\n",
    "combined_df['Item_Fat_Content'] = combined_df['Item_Fat_Content'].replace({'LF': 'Low Fat', 'reg': 'Regular', 'low fat': 'Low Fat'})\n",
    "\n",
    "# Encoding categorical variables\n",
    "cat_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n",
    "for col in cat_cols:\n",
    "    combined_df = pd.concat([combined_df, pd.get_dummies(combined_df[col], prefix=col)], axis=1)\n",
    "    combined_df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "combined_df.drop(['Item_Identifier', 'Outlet_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "\n",
    "# Split back into train and test\n",
    "train = combined_df[combined_df['source'] == 'train'].drop('source', axis=1)\n",
    "test = combined_df[combined_df['source'] == 'test'].drop(['source', 'Item_Outlet_Sales'], axis=1)\n",
    "\n",
    "# Prepare features and target\n",
    "X = train.drop('Item_Outlet_Sales', axis=1)\n",
    "y = train['Item_Outlet_Sales']\n",
    "X_test = test\n",
    "\n",
    "# Scale numerical columns with RobustScaler\n",
    "num_cols = ['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Age']\n",
    "scaler = RobustScaler()\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "# Define the base model for RFE\n",
    "base_model = ExtraTreesRegressor(n_estimators=472, max_depth=8, min_samples_leaf=40, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Set number of features to keep (half of total features)\n",
    "n_features = X.shape[1] // 2\n",
    "\n",
    "# Set up RFE\n",
    "selector = RFE(estimator=base_model, n_features_to_select=n_features, step=1)\n",
    "\n",
    "# Fit and transform your data\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X.columns[selector.get_support()].tolist()\n",
    "print(f\"Number of features selected: {len(selected_features)}\")\n",
    "print(f\"Selected features: {selected_features}\")\n",
    "\n",
    "# Split train data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define objective function for Optuna\n",
    "def objective(trial):\n",
    "    model_name = trial.suggest_categorical(\"model\", [\n",
    "        \"RandomForest\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \"ExtraTrees\", \"GradientBoosting\"\n",
    "    ])\n",
    "    \n",
    "    if model_name == \"RandomForest\":\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('rf_n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('rf_max_depth', 3, 15),\n",
    "            'min_samples_leaf': trial.suggest_int('rf_min_samples_leaf', 10, 100),\n",
    "            'n_jobs': -1,\n",
    "            'random_state': 42\n",
    "        }\n",
    "        model = RandomForestRegressor(**params)\n",
    "    \n",
    "    elif model_name == \"XGBoost\":\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('xgb_n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('xgb_max_depth', 3, 15),\n",
    "            'learning_rate': trial.suggest_float('xgb_learning_rate', 0.01, 0.3),\n",
    "            'reg_alpha': trial.suggest_float('xgb_reg_alpha', 0, 1),\n",
    "            'reg_lambda': trial.suggest_float('xgb_reg_lambda', 0, 1),\n",
    "            'random_state': 42\n",
    "        }\n",
    "        model = XGBRegressor(**params)\n",
    "    \n",
    "    elif model_name == \"LightGBM\":\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('lgb_n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('lgb_max_depth', 3, 15),\n",
    "            'learning_rate': trial.suggest_float('lgb_learning_rate', 0.01, 0.3),\n",
    "            'reg_alpha': trial.suggest_float('lgb_reg_alpha', 0, 1),\n",
    "            'reg_lambda': trial.suggest_float('lgb_reg_lambda', 0, 1),\n",
    "            'random_state': 42,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        model = LGBMRegressor(**params)\n",
    "    \n",
    "    elif model_name == \"CatBoost\":\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('cat_iterations', 100, 1000),\n",
    "            'depth': trial.suggest_int('cat_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('cat_learning_rate', 0.01, 0.3),\n",
    "            'l2_leaf_reg': trial.suggest_float('cat_l2_leaf_reg', 1, 10),\n",
    "            'random_seed': 42,\n",
    "            'verbose': 0\n",
    "        }\n",
    "        model = CatBoostRegressor(**params)\n",
    "    \n",
    "    elif model_name == \"ExtraTrees\":\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('et_n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('et_max_depth', 3, 15),\n",
    "            'min_samples_leaf': trial.suggest_int('et_min_samples_leaf', 10, 100),\n",
    "            'n_jobs': -1,\n",
    "            'random_state': 42\n",
    "        }\n",
    "        model = ExtraTreesRegressor(**params)\n",
    "    \n",
    "    elif model_name == \"GradientBoosting\":\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('gb_n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('gb_max_depth', 3, 15),\n",
    "            'learning_rate': trial.suggest_float('gb_learning_rate', 0.01, 0.3),\n",
    "            'min_samples_leaf': trial.suggest_int('gb_min_samples_leaf', 10, 100),\n",
    "            'random_state': 42\n",
    "        }\n",
    "        model = GradientBoostingRegressor(**params)\n",
    "\n",
    "    # Train and evaluate model with cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    trial.set_user_attr('params', {k: v for k, v in trial.params.items() if k != 'model'})\n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# Optimize with Optuna using TPESampler\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Get top trials and identify best model per algorithm\n",
    "trials = sorted(study.trials, key=lambda t: t.value if t.value is not None else float('inf'))\n",
    "algorithm_best_trials = {}\n",
    "for trial in trials:\n",
    "    model_name = trial.params['model']\n",
    "    if model_name not in algorithm_best_trials:  # Keep only the first (best) trial for each algorithm\n",
    "        algorithm_best_trials[model_name] = trial\n",
    "    if len(algorithm_best_trials) == 3:  # Stop after getting top 3 algorithms\n",
    "        break\n",
    "\n",
    "# Initialize models with best parameters from top 3 algorithms\n",
    "models = []\n",
    "for model_name, trial in algorithm_best_trials.items():\n",
    "    # Remove model-specific prefix from parameter names\n",
    "    params = {k.split('_', 1)[1] if '_' in k else k: v for k, v in trial.params.items() if k != 'model'}\n",
    "    \n",
    "    if model_name == \"RandomForest\":\n",
    "        model = RandomForestRegressor(**params, n_jobs=-1, random_state=42)\n",
    "    elif model_name == \"XGBoost\":\n",
    "        model = XGBRegressor(**params, random_state=42)\n",
    "    elif model_name == \"LightGBM\":\n",
    "        model = LGBMRegressor(**params, random_state=42, verbose=-1)\n",
    "    elif model_name == \"CatBoost\":\n",
    "        if 'iterations' in params:\n",
    "            params['iterations'] = params.pop('iterations')\n",
    "        model = CatBoostRegressor(**params, random_seed=42, verbose=0)\n",
    "    elif model_name == \"ExtraTrees\":\n",
    "        model = ExtraTreesRegressor(**params, n_jobs=-1, random_state=42)\n",
    "    elif model_name == \"GradientBoosting\":\n",
    "        model = GradientBoostingRegressor(**params, random_state=42)\n",
    "    \n",
    "    models.append((model_name, model))\n",
    "\n",
    "print(\"Top models from 3 distinct algorithms:\")\n",
    "for model_name, trial in algorithm_best_trials.items():\n",
    "    print(f\"Model: {model_name}, RMSE: {trial.value}, Params: {trial.user_attrs['params']}\")\n",
    "\n",
    "# Stacking with top models from top 3 algorithms\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_preds = {f\"{name}_{i}\": np.zeros(len(X_selected)) for i, (name, _) in enumerate(models)}\n",
    "test_preds = {f\"{name}_{i}\": np.zeros(len(X_test_selected)) for i, (name, _) in enumerate(models)}\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_selected):\n",
    "    X_tr, X_val = X_selected[train_idx], X_selected[val_idx]\n",
    "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    for i, (model_name, model) in enumerate(models):\n",
    "        model.fit(X_tr, y_tr)\n",
    "        oof_preds[f\"{model_name}_{i}\"][val_idx] = model.predict(X_val)\n",
    "        test_preds[f\"{model_name}_{i}\"] += model.predict(X_test_selected) / kf.n_splits\n",
    "\n",
    "# Meta-model (Ridge Regression)\n",
    "meta_features = np.column_stack([oof_preds[key] for key in oof_preds])\n",
    "meta_model = Ridge(alpha=1.0)\n",
    "meta_model.fit(meta_features, y)\n",
    "\n",
    "# Evaluate on validation set\n",
    "meta_val_preds = meta_model.predict(np.column_stack([model.predict(X_val) for _, model in models]))\n",
    "rmse = np.sqrt(mean_squared_error(y_val, meta_val_preds))\n",
    "print(f\"Validation RMSE with stacking of top models from top 3 algorithms: {rmse}\")\n",
    "\n",
    "# Final test predictions\n",
    "meta_test_preds = meta_model.predict(np.column_stack([test_preds[key] for key in test_preds]))\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame({\n",
    "    'Item_Identifier': test_df['Item_Identifier'],\n",
    "    'Outlet_Identifier': test_df['Outlet_Identifier'],\n",
    "    'Item_Outlet_Sales': meta_test_preds\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created: submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross-Validation RMSE Scores: [1117.0704148038217, 1075.8356166754372, 1049.429781718685, 1069.085988629873, 1068.4921425557623]\n",
      "Average RMSE: 1075.9827888767159\n",
      "Standard Deviation of RMSE: 22.347046735832947\n",
      "Submission file created: submission_extra_trees.csv\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('train_v9rqX0R.csv')  # Replace with actual train file path\n",
    "train_df = train_df[~train_df['Item_Identifier'].isin([\n",
    "    'FDX20', 'FDG33', 'FDW13', 'FDG24', 'DRE49', 'NCY18',\n",
    "    'FDO19', 'FDL34', 'FDO52', 'NCL31', 'FDA04', 'NCQ06',\n",
    "    'FDT07', 'FDL10', 'FDX04', 'FDU19'])]\n",
    "test_df = pd.read_csv('test_AbJTz2l.csv')    # Replace with actual test file path\n",
    "\n",
    "# Combine train and test for preprocessing\n",
    "train_df['source'] = 'train'\n",
    "test_df['source'] = 'test'\n",
    "combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Data Preprocessing\n",
    "combined_df['Item_Weight'] = combined_df.groupby('Item_Type')['Item_Weight'].transform(lambda x: x.fillna(x.mean()))\n",
    "combined_df['Outlet_Size'] = combined_df.groupby('Outlet_Type')['Outlet_Size'].transform(\n",
    "    lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'Medium'))\n",
    "item_visibility_mean = combined_df[combined_df['Item_Visibility'] > 0].groupby('Item_Identifier')['Item_Visibility'].mean()\n",
    "combined_df['Item_Visibility'] = combined_df.apply(\n",
    "    lambda row: item_visibility_mean[row['Item_Identifier']] if row['Item_Visibility'] == 0 else row['Item_Visibility'],\n",
    "    axis=1\n",
    ")\n",
    "combined_df['Item_Visibility'].fillna(combined_df['Item_Visibility'].mean(), inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "combined_df['Outlet_Age'] = 2025 - combined_df['Outlet_Establishment_Year']\n",
    "combined_df['Item_Fat_Content'] = combined_df['Item_Fat_Content'].replace({'LF': 'Low Fat', 'reg': 'Regular', 'low fat': 'Low Fat'})\n",
    "\n",
    "# Encoding categorical variables (only for selected features that need encoding)\n",
    "combined_df = pd.concat([combined_df, pd.get_dummies(combined_df['Item_Fat_Content'], prefix='Item_Fat_Content')], axis=1)\n",
    "combined_df = pd.concat([combined_df, pd.get_dummies(combined_df['Item_Type'], prefix='Item_Type')], axis=1)\n",
    "combined_df = pd.concat([combined_df, pd.get_dummies(combined_df['Outlet_Type'], prefix='Outlet_Type')], axis=1)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "combined_df.drop(['Item_Identifier', 'Outlet_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "\n",
    "# Split back into train and test\n",
    "train = combined_df[combined_df['source'] == 'train'].drop('source', axis=1)\n",
    "test = combined_df[combined_df['source'] == 'test'].drop(['source', 'Item_Outlet_Sales'], axis=1)\n",
    "\n",
    "# Selected features\n",
    "selected_features = [\n",
    "    'Item_MRP', \n",
    "    'Item_Fat_Content_Low Fat', \n",
    "    'Item_Type_Fruits and Vegetables', \n",
    "    'Item_Type_Seafood', \n",
    "    'Outlet_Type_Grocery Store', \n",
    "    'Outlet_Type_Supermarket Type2', \n",
    "    'Outlet_Type_Supermarket Type3'\n",
    "]\n",
    "\n",
    "# Prepare features and target\n",
    "X = train[selected_features]\n",
    "y = train['Item_Outlet_Sales']\n",
    "X_test = test[selected_features]\n",
    "\n",
    "# Scale numerical columns with RobustScaler (only Item_MRP is numerical among selected features)\n",
    "scaler = RobustScaler()\n",
    "X[['Item_MRP']] = scaler.fit_transform(X[['Item_MRP']])\n",
    "X_test[['Item_MRP']] = scaler.transform(X_test[['Item_MRP']])\n",
    "\n",
    "# Initialize ExtraTreesRegressor\n",
    "model = ExtraTreesRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=20,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_preds = np.zeros(len(X))\n",
    "test_preds = np.zeros(len(X_test))\n",
    "rmse_scores = []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    val_preds = model.predict(X_val)\n",
    "    oof_preds[val_idx] = val_preds\n",
    "    \n",
    "    # Calculate RMSE for this fold\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "    rmse_scores.append(rmse)\n",
    "    \n",
    "    # Predict on test set (average across folds)\n",
    "    test_preds += model.predict(X_test) / kf.n_splits\n",
    "\n",
    "# Print results\n",
    "print(f\"5-Fold Cross-Validation RMSE Scores: {rmse_scores}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores)}\")\n",
    "print(f\"Standard Deviation of RMSE: {np.std(rmse_scores)}\")\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame({\n",
    "    'Item_Identifier': test_df['Item_Identifier'],\n",
    "    'Outlet_Identifier': test_df['Outlet_Identifier'],\n",
    "    'Item_Outlet_Sales': test_preds\n",
    "})\n",
    "submission.to_csv('submission_extra_trees.csv', index=False)\n",
    "print(\"Submission file created: submission_extra_trees.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
