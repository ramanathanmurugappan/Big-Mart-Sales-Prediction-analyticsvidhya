{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Processing training set...\n",
      "(8523, 13)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from preprocessing_utils import (\n",
    "    identify_column_types,\n",
    "    standardize_features,\n",
    "    fit_impute_item_weight,\n",
    "    transform_impute_item_weight,\n",
    "    fit_mice_imputation,\n",
    "    transform_mice_imputation,\n",
    "    create_features,\n",
    "    detect_and_remove_outliers,\n",
    "    transform_features,\n",
    "    fit_scale_numerical_features,\n",
    "    transform_scale_numerical_features,\n",
    "    fit_encode_categorical_features,\n",
    "    transform_encode_categorical_features\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv('train_v9rqX0R.csv')\n",
    "test_df = pd.read_csv('test_AbJTz2l.csv')\n",
    "\n",
    "print(\"Processing training set...\")\n",
    "# Step 1: Initial preprocessing\n",
    "numerical_cols, categorical_cols = identify_column_types(train_df)\n",
    "train_df = standardize_features(train_df)\n",
    "\n",
    "print(train_df.shape)\n",
    "original_columns = set(train_df.columns.to_list())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8523, 13)\n",
      "Missing columns: set()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Weight imputation\n",
    "train_df, item_weights = fit_impute_item_weight(train_df)\n",
    "\n",
    "\n",
    "\n",
    "# Identify missing columns\n",
    "missing_columns = original_columns - set(train_df.columns.to_list())\n",
    "print(train_df.shape)\n",
    "print(\"Missing columns:\", missing_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8523, 13)\n",
      "Missing columns: set()\n"
     ]
    }
   ],
   "source": [
    "# Step 3: MICE imputation\n",
    "selected_columns = [\n",
    "    \"Item_Identifier\", \"Item_Weight\", \"Item_Fat_Content\", \"Item_Visibility\",\n",
    "    \"Item_Type\", \"Item_MRP\", \"Outlet_Identifier\", \"Outlet_Establishment_Year\",\n",
    "    \"Outlet_Size\", \"Outlet_Location_Type\", \"Outlet_Type\",\"Item_Type_Grouped\"]\n",
    "categorical_vars = selected_columns\n",
    "train_df, mice_mappings = fit_mice_imputation(train_df, selected_columns, categorical_vars)\n",
    "\n",
    "# Identify missing columns\n",
    "missing_columns = original_columns - set(train_df.columns.to_list())\n",
    "print(train_df.shape)\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8523, 18)\n",
      "Missing columns: set()\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Feature engineering\n",
    "train_df = create_features(train_df)\n",
    "train_df = transform_features(train_df)\n",
    "\n",
    "\n",
    "# Identify missing columns\n",
    "missing_columns = original_columns - set(train_df.columns.to_list())\n",
    "print(train_df.shape)\n",
    "print(\"Missing columns:\", missing_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8157, 18)\n",
      "Missing columns: set()\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Outlier detection and removal\n",
    "numerical_cols, categorical_cols= identify_column_types(train_df)\n",
    "train_df = detect_and_remove_outliers(train_df, test_df, numerical_cols)\n",
    "\n",
    "# Identify missing columns\n",
    "missing_columns = original_columns - set(train_df.columns.to_list())\n",
    "print(train_df.shape)\n",
    "print(\"Missing columns:\", missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8157, 18)\n",
      "Missing columns: set()\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Scaling\n",
    "numerical_cols, categorical_cols= identify_column_types(train_df)\n",
    "train_df, scaler = fit_scale_numerical_features(train_df, numerical_cols)\n",
    "\n",
    "# Identify missing columns\n",
    "missing_columns = original_columns - set(train_df.columns.to_list())\n",
    "print(train_df.shape)\n",
    "print(\"Missing columns:\", missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8157, 18)\n",
      "Missing columns: set()\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Encoding\n",
    "train_df, encoder = fit_encode_categorical_features(train_df)\n",
    "\n",
    "# Identify missing columns\n",
    "missing_columns = original_columns - set(train_df.columns.to_list())\n",
    "print(train_df.shape)\n",
    "print(\"Missing columns:\", missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Item_Identifier',\n",
       " 'Item_Weight',\n",
       " 'Item_Fat_Content',\n",
       " 'Item_Visibility',\n",
       " 'Item_Type',\n",
       " 'Item_MRP',\n",
       " 'Outlet_Identifier',\n",
       " 'Outlet_Establishment_Year',\n",
       " 'Outlet_Size',\n",
       " 'Outlet_Location_Type',\n",
       " 'Outlet_Type',\n",
       " 'Item_Outlet_Sales',\n",
       " 'Item_Type_Grouped',\n",
       " 'Outlet_Age',\n",
       " 'Establishment_Decade',\n",
       " 'Outlet_Age_Category',\n",
       " 'Price_per_Unit_Weight',\n",
       " 'Item_MRP_Binned']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing test set...\n",
      "\n",
      "Saving processed datasets...\n",
      "Done! Processed datasets saved as 'processed_train.csv' and 'processed_test.csv'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nProcessing test set...\")\n",
    "# Process test set using fitted transformations but without outlier removal\n",
    "test_df = standardize_features(test_df)\n",
    "test_df = transform_impute_item_weight(test_df, item_weights)\n",
    "numerical_cols, categorical_cols = identify_column_types(test_df)\n",
    "test_df = transform_mice_imputation(test_df, selected_columns, categorical_vars, mice_mappings)\n",
    "test_df = create_features(test_df)\n",
    "test_df = transform_features(test_df)\n",
    "numerical_cols, categorical_cols = identify_column_types(test_df)\n",
    "test_df = transform_scale_numerical_features(test_df, numerical_cols, scaler)\n",
    "test_df = transform_encode_categorical_features(test_df, encoder)\n",
    "\n",
    "print(\"\\nSaving processed datasets...\")\n",
    "train_df.to_csv('processed_train.csv', index=False)\n",
    "test_df.to_csv('processed_test.csv', index=False)\n",
    "print(\"Done! Processed datasets saved as 'processed_train.csv' and 'processed_test.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Item_Identifier', 'Item_Weight', 'Item_Fat_Content', 'Item_Visibility', 'Item_Type', 'Item_MRP', 'Outlet_Identifier', 'Outlet_Establishment_Year', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type', 'Item_Outlet_Sales', 'Item_Type_Grouped', 'Outlet_Age', 'Establishment_Decade', 'Outlet_Age_Category', 'Price_per_Unit_Weight', 'Item_MRP_Binned']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error # compute RMSE by utilizing the mean_squared_error function with the squared parameter set to False\n",
    "from optuna.samplers import TPESampler\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Model Training and Evaluation\n",
    "# =============================================================================\n",
    "\n",
    "training_data = pd.read_csv('processed_train.csv')\n",
    "testing_data = pd.read_csv('processed_test.csv')\n",
    "\n",
    "\n",
    "print(training_data.columns.tolist())\n",
    "\n",
    "# Define features and target\n",
    "X = training_data.drop(columns=['Item_Outlet_Sales','Item_Identifier'])\n",
    "y = training_data['Item_Outlet_Sales']\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-Learn Version: 1.5.2\n",
      "XGBoost Version: 2.1.3\n",
      "LightGBM Version: 4.5.0\n",
      "CatBoost Version: 1.2.7\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost\n",
    "\n",
    "print(\"Scikit-Learn Version:\", sklearn.__version__)\n",
    "print(\"XGBoost Version:\", xgb.__version__)\n",
    "print(\"LightGBM Version:\", lgb.__version__)\n",
    "print(\"CatBoost Version:\", catboost.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running RandomizedSearchCV for RandomForest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV:  20%|██        | 1/5 [00:21<01:24, 21.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running RandomizedSearchCV for XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV:  40%|████      | 2/5 [00:22<00:29,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running RandomizedSearchCV for LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004333 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004387 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Info] Start training from score 7.402653[LightGBM] [Info] Start training from score 7.398504\n",
      "\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002995 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003643 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003601 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003373 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002643 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003243 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002948 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002639 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002834 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002636 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002786 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003250 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002774 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1048\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398504\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002661 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.402653\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1049\n",
      "[LightGBM] [Info] Number of data points in the train set: 3806, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.398010\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1050\n",
      "[LightGBM] [Info] Number of data points in the train set: 5709, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 7.399722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV:  60%|██████    | 3/5 [01:13<00:57, 28.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running RandomizedSearchCV for CatBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV:  80%|████████  | 4/5 [01:18<00:19, 19.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running RandomizedSearchCV for ExtraTrees...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV: 100%|██████████| 5/5 [01:26<00:00, 17.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomizedSearchCV Results Saved: 'random_search_results.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'n_estimators': 100, 'min_samples_split': 10,...</td>\n",
       "      <td>0.526283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>{'subsample': 0.8, 'n_estimators': 100, 'max_d...</td>\n",
       "      <td>0.520316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'num_leaves': 20, 'n_estimators': 200, 'max_d...</td>\n",
       "      <td>0.528208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'learning_rate': 0.05, 'l2_leaf_reg': 3, 'ite...</td>\n",
       "      <td>0.516475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>{'n_estimators': 50, 'min_samples_split': 5, '...</td>\n",
       "      <td>0.522938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model                                        Best Params      RMSE\n",
       "0  RandomForest  {'n_estimators': 100, 'min_samples_split': 10,...  0.526283\n",
       "1       XGBoost  {'subsample': 0.8, 'n_estimators': 100, 'max_d...  0.520316\n",
       "2      LightGBM  {'num_leaves': 20, 'n_estimators': 200, 'max_d...  0.528208\n",
       "3      CatBoost  {'learning_rate': 0.05, 'l2_leaf_reg': 3, 'ite...  0.516475\n",
       "4    ExtraTrees  {'n_estimators': 50, 'min_samples_split': 5, '...  0.522938"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(),\n",
    "    \"XGBoost\": xgb.XGBRegressor(objective=\"reg:squarederror\"),\n",
    "    \"LightGBM\": lgb.LGBMRegressor(),\n",
    "    \"CatBoost\": CatBoostRegressor(verbose=0),\n",
    "    \"ExtraTrees\": ExtraTreesRegressor()\n",
    "}\n",
    "\n",
    "# Define hyperparameter grids\n",
    "param_grids = {\n",
    "    \"RandomForest\": {\n",
    "        \"n_estimators\": [50, 100, 200, 500],\n",
    "        \"max_depth\": [None, 10, 20, 30],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"max_depth\": [3, 6, 9],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"subsample\": [0.7, 0.8, 1.0]\n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"num_leaves\": [20, 30, 40],\n",
    "        \"max_depth\": [-1, 10, 20]\n",
    "    },\n",
    "    \"CatBoost\": {\n",
    "        \"iterations\": [100, 200, 300],\n",
    "        \"depth\": [4, 6, 10],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"l2_leaf_reg\": [3, 5, 7]\n",
    "    },\n",
    "    \"ExtraTrees\": {\n",
    "        \"n_estimators\": [50, 100, 200, 500],\n",
    "        \"max_depth\": [None, 10, 20, 30],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Store results in DataFrame\n",
    "results = []\n",
    "\n",
    "for model_name in tqdm(models, desc=\"Running RandomizedSearchCV\"):\n",
    "    print(f\"\\nRunning RandomizedSearchCV for {model_name}...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        models[model_name], \n",
    "        param_distributions=param_grids[model_name], \n",
    "        n_iter=20, \n",
    "        cv=3, \n",
    "        scoring=\"neg_mean_squared_error\", \n",
    "        random_state=42, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = search.best_estimator_\n",
    "    best_params = search.best_params_\n",
    "    preds = best_model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "\n",
    "    results.append({\"Model\": model_name, \"Best Params\": best_params, \"RMSE\": rmse})\n",
    "\n",
    "# Convert results to DataFrame and save\n",
    "df_results_random = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "df_results_random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'n_estimators': 100, 'min_samples_split': 10,...</td>\n",
       "      <td>0.526283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>{'subsample': 0.8, 'n_estimators': 100, 'max_d...</td>\n",
       "      <td>0.520316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'num_leaves': 20, 'n_estimators': 200, 'max_d...</td>\n",
       "      <td>0.528208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'learning_rate': 0.05, 'l2_leaf_reg': 3, 'ite...</td>\n",
       "      <td>0.516475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>{'n_estimators': 50, 'min_samples_split': 5, '...</td>\n",
       "      <td>0.522938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model                                        Best Params      RMSE\n",
       "0  RandomForest  {'n_estimators': 100, 'min_samples_split': 10,...  0.526283\n",
       "1       XGBoost  {'subsample': 0.8, 'n_estimators': 100, 'max_d...  0.520316\n",
       "2      LightGBM  {'num_leaves': 20, 'n_estimators': 200, 'max_d...  0.528208\n",
       "3      CatBoost  {'learning_rate': 0.05, 'l2_leaf_reg': 3, 'ite...  0.516475\n",
       "4    ExtraTrees  {'n_estimators': 50, 'min_samples_split': 5, '...  0.522938"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_results_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Store results\n",
    "results_optuna = []\n",
    "\n",
    "n_trials = 50\n",
    "pbar = tqdm(total=n_trials, desc=\"Optimizing\")\n",
    "\n",
    "def objective(trial):\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"RandomForest\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \"ExtraTrees\"])\n",
    "    \n",
    "    if model_name == \"RandomForest\":\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "            max_depth=trial.suggest_int(\"max_depth\", 1, 30), \n",
    "            min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 1, 4),\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"XGBoost\":\n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=trial.suggest_int(\"n_estimators\", 100, 300),\n",
    "            max_depth=trial.suggest_int(\"max_depth\", 3, 9),  \n",
    "            learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "            subsample=trial.suggest_float(\"subsample\", 0.7, 1.0),\n",
    "            objective=\"reg:squarederror\",\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"LightGBM\":\n",
    "        model = lgb.LGBMRegressor(\n",
    "            n_estimators=trial.suggest_int(\"n_estimators\", 100, 300),\n",
    "            learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "            num_leaves=trial.suggest_int(\"num_leaves\", 20, 40),\n",
    "            max_depth=trial.suggest_int(\"max_depth\", -1, 20),  \n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"CatBoost\":\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=trial.suggest_int(\"iterations\", 100, 300),\n",
    "            depth=trial.suggest_int(\"depth\", 4, 10),\n",
    "            learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n",
    "            l2_leaf_reg=trial.suggest_int(\"l2_leaf_reg\", 3, 7),\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"ExtraTrees\":\n",
    "        model = ExtraTreesRegressor(\n",
    "            n_estimators=trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "            max_depth=trial.suggest_int(\"max_depth\", 1, 30), \n",
    "            min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 1, 4),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "\n",
    "    results_optuna.append({\"Model\": model_name, \"Best Params\": trial.params, \"RMSE\": rmse})\n",
    "    pbar.update(1)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Convert results to DataFrame and save\n",
    "df_results_optuna = pd.DataFrame(results_optuna)\n",
    "\n",
    "df_results_optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 215, 'dept...</td>\n",
       "      <td>0.520592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'model': 'RandomForest', 'n_estimators': 329,...</td>\n",
       "      <td>0.530089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'model': 'LightGBM', 'n_estimators': 141, 'le...</td>\n",
       "      <td>0.540498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'model': 'LightGBM', 'n_estimators': 270, 'le...</td>\n",
       "      <td>0.536452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'model': 'RandomForest', 'n_estimators': 290,...</td>\n",
       "      <td>0.533550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'model': 'LightGBM', 'n_estimators': 208, 'le...</td>\n",
       "      <td>0.563068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'model': 'RandomForest', 'n_estimators': 450,...</td>\n",
       "      <td>0.535583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>{'model': 'XGBoost', 'n_estimators': 250, 'max...</td>\n",
       "      <td>0.544326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'model': 'LightGBM', 'n_estimators': 173, 'le...</td>\n",
       "      <td>0.536043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 164, 'dept...</td>\n",
       "      <td>0.518198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 102, 'dept...</td>\n",
       "      <td>0.581563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 224, 'dept...</td>\n",
       "      <td>0.518382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>{'model': 'ExtraTrees', 'n_estimators': 58, 'm...</td>\n",
       "      <td>0.532717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 182, 'dept...</td>\n",
       "      <td>0.517861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 148, 'dept...</td>\n",
       "      <td>0.517782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 149, 'dept...</td>\n",
       "      <td>0.550359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>{'model': 'XGBoost', 'n_estimators': 103, 'max...</td>\n",
       "      <td>0.519673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>{'model': 'ExtraTrees', 'n_estimators': 496, '...</td>\n",
       "      <td>0.542547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 291, 'dept...</td>\n",
       "      <td>0.517477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 300, 'dept...</td>\n",
       "      <td>0.516588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 298, 'dept...</td>\n",
       "      <td>0.518220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 295, 'dept...</td>\n",
       "      <td>0.516175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 296, 'dept...</td>\n",
       "      <td>0.516119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 261, 'dept...</td>\n",
       "      <td>0.516455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 259, 'dept...</td>\n",
       "      <td>0.516959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>{'model': 'ExtraTrees', 'n_estimators': 355, '...</td>\n",
       "      <td>0.537367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>{'model': 'XGBoost', 'n_estimators': 204, 'max...</td>\n",
       "      <td>0.531426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 259, 'dept...</td>\n",
       "      <td>0.516921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 261, 'dept...</td>\n",
       "      <td>0.516458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 271, 'dept...</td>\n",
       "      <td>0.516623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 230, 'dept...</td>\n",
       "      <td>0.516888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 272, 'dept...</td>\n",
       "      <td>0.516166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 277, 'dept...</td>\n",
       "      <td>0.516515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'model': 'RandomForest', 'n_estimators': 390,...</td>\n",
       "      <td>0.525054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 248, 'dept...</td>\n",
       "      <td>0.516588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 282, 'dept...</td>\n",
       "      <td>0.517606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'model': 'LightGBM', 'n_estimators': 178, 'le...</td>\n",
       "      <td>0.522990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'model': 'RandomForest', 'n_estimators': 423,...</td>\n",
       "      <td>0.536255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>{'model': 'XGBoost', 'n_estimators': 223, 'max...</td>\n",
       "      <td>0.525335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>{'model': 'ExtraTrees', 'n_estimators': 50, 'm...</td>\n",
       "      <td>0.533671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'model': 'LightGBM', 'n_estimators': 139, 'le...</td>\n",
       "      <td>0.523813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 243, 'dept...</td>\n",
       "      <td>0.516817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 274, 'dept...</td>\n",
       "      <td>0.516653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 263, 'dept...</td>\n",
       "      <td>0.521393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 287, 'dept...</td>\n",
       "      <td>0.516469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'model': 'RandomForest', 'n_estimators': 498,...</td>\n",
       "      <td>0.524990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 240, 'dept...</td>\n",
       "      <td>0.516890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 283, 'dept...</td>\n",
       "      <td>0.516815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'model': 'CatBoost', 'iterations': 257, 'dept...</td>\n",
       "      <td>0.516926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'model': 'LightGBM', 'n_estimators': 237, 'le...</td>\n",
       "      <td>0.539363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model                                        Best Params      RMSE\n",
       "0       CatBoost  {'model': 'CatBoost', 'iterations': 215, 'dept...  0.520592\n",
       "1   RandomForest  {'model': 'RandomForest', 'n_estimators': 329,...  0.530089\n",
       "2       LightGBM  {'model': 'LightGBM', 'n_estimators': 141, 'le...  0.540498\n",
       "3       LightGBM  {'model': 'LightGBM', 'n_estimators': 270, 'le...  0.536452\n",
       "4   RandomForest  {'model': 'RandomForest', 'n_estimators': 290,...  0.533550\n",
       "5       LightGBM  {'model': 'LightGBM', 'n_estimators': 208, 'le...  0.563068\n",
       "6   RandomForest  {'model': 'RandomForest', 'n_estimators': 450,...  0.535583\n",
       "7        XGBoost  {'model': 'XGBoost', 'n_estimators': 250, 'max...  0.544326\n",
       "8       LightGBM  {'model': 'LightGBM', 'n_estimators': 173, 'le...  0.536043\n",
       "9       CatBoost  {'model': 'CatBoost', 'iterations': 164, 'dept...  0.518198\n",
       "10      CatBoost  {'model': 'CatBoost', 'iterations': 102, 'dept...  0.581563\n",
       "11      CatBoost  {'model': 'CatBoost', 'iterations': 224, 'dept...  0.518382\n",
       "12    ExtraTrees  {'model': 'ExtraTrees', 'n_estimators': 58, 'm...  0.532717\n",
       "13      CatBoost  {'model': 'CatBoost', 'iterations': 182, 'dept...  0.517861\n",
       "14      CatBoost  {'model': 'CatBoost', 'iterations': 148, 'dept...  0.517782\n",
       "15      CatBoost  {'model': 'CatBoost', 'iterations': 149, 'dept...  0.550359\n",
       "16       XGBoost  {'model': 'XGBoost', 'n_estimators': 103, 'max...  0.519673\n",
       "17    ExtraTrees  {'model': 'ExtraTrees', 'n_estimators': 496, '...  0.542547\n",
       "18      CatBoost  {'model': 'CatBoost', 'iterations': 291, 'dept...  0.517477\n",
       "19      CatBoost  {'model': 'CatBoost', 'iterations': 300, 'dept...  0.516588\n",
       "20      CatBoost  {'model': 'CatBoost', 'iterations': 298, 'dept...  0.518220\n",
       "21      CatBoost  {'model': 'CatBoost', 'iterations': 295, 'dept...  0.516175\n",
       "22      CatBoost  {'model': 'CatBoost', 'iterations': 296, 'dept...  0.516119\n",
       "23      CatBoost  {'model': 'CatBoost', 'iterations': 261, 'dept...  0.516455\n",
       "24      CatBoost  {'model': 'CatBoost', 'iterations': 259, 'dept...  0.516959\n",
       "25    ExtraTrees  {'model': 'ExtraTrees', 'n_estimators': 355, '...  0.537367\n",
       "26       XGBoost  {'model': 'XGBoost', 'n_estimators': 204, 'max...  0.531426\n",
       "27      CatBoost  {'model': 'CatBoost', 'iterations': 259, 'dept...  0.516921\n",
       "28      CatBoost  {'model': 'CatBoost', 'iterations': 261, 'dept...  0.516458\n",
       "29      CatBoost  {'model': 'CatBoost', 'iterations': 271, 'dept...  0.516623\n",
       "30      CatBoost  {'model': 'CatBoost', 'iterations': 230, 'dept...  0.516888\n",
       "31      CatBoost  {'model': 'CatBoost', 'iterations': 272, 'dept...  0.516166\n",
       "32      CatBoost  {'model': 'CatBoost', 'iterations': 277, 'dept...  0.516515\n",
       "33  RandomForest  {'model': 'RandomForest', 'n_estimators': 390,...  0.525054\n",
       "34      CatBoost  {'model': 'CatBoost', 'iterations': 248, 'dept...  0.516588\n",
       "35      CatBoost  {'model': 'CatBoost', 'iterations': 282, 'dept...  0.517606\n",
       "36      LightGBM  {'model': 'LightGBM', 'n_estimators': 178, 'le...  0.522990\n",
       "37  RandomForest  {'model': 'RandomForest', 'n_estimators': 423,...  0.536255\n",
       "38       XGBoost  {'model': 'XGBoost', 'n_estimators': 223, 'max...  0.525335\n",
       "39    ExtraTrees  {'model': 'ExtraTrees', 'n_estimators': 50, 'm...  0.533671\n",
       "40      LightGBM  {'model': 'LightGBM', 'n_estimators': 139, 'le...  0.523813\n",
       "41      CatBoost  {'model': 'CatBoost', 'iterations': 243, 'dept...  0.516817\n",
       "42      CatBoost  {'model': 'CatBoost', 'iterations': 274, 'dept...  0.516653\n",
       "43      CatBoost  {'model': 'CatBoost', 'iterations': 263, 'dept...  0.521393\n",
       "44      CatBoost  {'model': 'CatBoost', 'iterations': 287, 'dept...  0.516469\n",
       "45  RandomForest  {'model': 'RandomForest', 'n_estimators': 498,...  0.524990\n",
       "46      CatBoost  {'model': 'CatBoost', 'iterations': 240, 'dept...  0.516890\n",
       "47      CatBoost  {'model': 'CatBoost', 'iterations': 283, 'dept...  0.516815\n",
       "48      CatBoost  {'model': 'CatBoost', 'iterations': 257, 'dept...  0.516926\n",
       "49      LightGBM  {'model': 'LightGBM', 'n_estimators': 237, 'le...  0.539363"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results, CatBoost appears to be the best model as it consistently achieves the lowest RMSE across different parameter tuning methods. The best-performing parameters from the random grid search and TPESampler are:\n",
    "\n",
    "Best CatBoost Model Parameters (from TPESampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Model RMSE: 0.5162\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "X = training_data.drop(columns=['Item_Outlet_Sales', 'Item_Identifier'])  # Drop unnecessary columns\n",
    "y = training_data['Item_Outlet_Sales']\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Best CatBoost parameters from TPESampler\n",
    "best_params = {\n",
    "    \"iterations\": 296,\n",
    "    \"depth\": 7,\n",
    "    \"learning_rate\": 0.021573131062485866,\n",
    "    \"l2_leaf_reg\": 4\n",
    "}\n",
    "\n",
    "# Initialize and train the CatBoost model\n",
    "catboost_model = CatBoostRegressor(**best_params, verbose=0)\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = catboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Print RMSE\n",
    "print(f\"CatBoost Model RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Model RMSE: 1141.0726\n"
     ]
    }
   ],
   "source": [
    "# Reverse log transformation (convert back to original scale)\n",
    "y_test = np.expm1(y_test)\n",
    "\n",
    "# Reverse log transformation (convert back to original scale)\n",
    "y_pred = np.expm1(y_pred)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Print RMSE\n",
    "print(f\"CatBoost Model RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
